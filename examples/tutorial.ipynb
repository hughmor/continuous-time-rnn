{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how this whole project will be improved\n",
    "\n",
    "organize into `RecurrentLayer` that can be composed with one another - similar to Nengo Populations of neurons\n",
    "each recurrent layer will have the all to all weight structure and is made up of a bunch of Neuron objects\n",
    "\n",
    "numerical solving\n",
    "\n",
    "1st idea i had:\n",
    "    do matmul at beginning of timestep\n",
    "    evaluate each individual neuron's dynamics with it's own solver (optimized methods could take different amount of steps in each neuron)\n",
    "\n",
    "--\n",
    "    would be good if there was a global state, perhaps this is the `Simulation` class (specific per-run object that stores simulation info and results)\n",
    "    so that each solver instance can access state via (neuron, time, type=[ y,s[i] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project assumes that you have a weight matrix and a network structure that you want to simulate.\n",
    "\n",
    "Perhaps future update could focus on more of the encoding/decoding/learning, but much of this is already done by nengo. There's a good chance that I find out later that nengo is able to do everything I want this project to do and more, but I want to develop this anyway, first because I want to, and also because I'm looking for something a little bit more special purpose for the research that I am doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 1.0\n",
    "weights = [\n",
    "    [W,-1],\n",
    "    [1,W],\n",
    "]\n",
    "n_neurons = len(weights)\n",
    "\n",
    "import butterpy as bpy\n",
    "from butterpy.layers import RecurrentLayer\n",
    "from butterpy.neurons import LowpassNeuron, YamadaNeuron\n",
    "\n",
    "stimulus = bpy.Node(function=lambda t: np.sin(2*np.pi*1000*t))\n",
    "layer = RecurrentLayer(N=n_neurons, neuron_params=[], neuron_type=YamadaNeuron)\n",
    "recurr = bpy.Connection()\n",
    "conn = bpy.Connection(stimulus, layer, )\n",
    "\n",
    "model = bpy.Model(layers=[layer], sim_time=)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ctrnn import CTRNN\n",
    "\n",
    "n_neurons = 24\n",
    "n_in = 3\n",
    "params = {\n",
    "    'number of neurons': n_neurons,\n",
    "    'number of inputs': n_in,\n",
    "    'number of outputs': n_neurons,\n",
    "    'decay constant': 0.01,\n",
    "    'weight matrix': np.random.random(size=(n_neurons,n_neurons)),\n",
    "    'input weights': np.array([\n",
    "        [1.,0.,0.],\n",
    "        [0.,2.,0.],\n",
    "        [0.,0.,3.]\n",
    "    ]),\n",
    "    'integration mode': 'RK4',\n",
    "    'activation': 'ReLU',\n",
    "    'initial state': np.ones(shape=n_neurons),\n",
    "    'biases': np.zeros(shape=n_neurons),\n",
    "}\n",
    "\n",
    "nn = CTRNN(**params)\n",
    "\n",
    "sim_time_seconds = 1.0\n",
    "N = 100\n",
    "freq = 5\n",
    "input1 = [np.sin(2. * np.pi * freq * (x/T) ) for x in range(N)]\n",
    "input2 = [np.cos(2. * np.pi * freq * (x/T) ) for x in range(N)]\n",
    "input3 = [one + two for one,two in zip(input1, input2)]\n",
    "inputs = [input1, input2, input3]\n",
    "\n",
    "dt = sim_time_seconds / N\n",
    "nn.simulate(inputs, dt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
